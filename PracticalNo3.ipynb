# Import libraries
import tensorflow as tf
from tensorflow.keras import layers, models, regularizers
import matplotlib.pyplot as plt

# 1️⃣ Load and normalize MNIST dataset
(x_train, y_train), (x_test, y_test) = tf.keras.datasets.mnist.load_data()
x_train, x_test = x_train / 255.0, x_test / 255.0

# 2️⃣ Define a helper function to create models
def create_model(use_l2=False, use_dropout=False):
    model = models.Sequential()
    model.add(layers.Flatten(input_shape=(28, 28)))

    # First hidden layer
    if use_l2:
        model.add(layers.Dense(256, activation='relu',
                               kernel_regularizer=regularizers.l2(0.001)))
    else:
        model.add(layers.Dense(256, activation='relu'))

    # Dropout layer (optional)
    if use_dropout:
        model.add(layers.Dropout(0.3))  # drops 30% of neurons randomly

    # Second hidden layer
    if use_l2:
        model.add(layers.Dense(128, activation='relu',
                               kernel_regularizer=regularizers.l2(0.001)))
    else:
        model.add(layers.Dense(128, activation='relu'))

    # Another Dropout if needed
    if use_dropout:
        model.add(layers.Dropout(0.3))

    # Output layer
    model.add(layers.Dense(10, activation='softmax'))

    # Compile model
    model.compile(optimizer='adam',
                  loss='sparse_categorical_crossentropy',
                  metrics=['accuracy'])
    return model


# 3️⃣ Create three models
model_base = create_model()  # no regularization
model_l2 = create_model(use_l2=True)  # with L2 regularization
model_dropout = create_model(use_dropout=True)  # with dropout

# 4️⃣ Train all models
print("Training Base Model...")
history_base = model_base.fit(x_train, y_train, epochs=5, batch_size=64,
                              validation_data=(x_test, y_test), verbose=1)

print("\nTraining L2 Regularized Model...")
history_l2 = model_l2.fit(x_train, y_train, epochs=5, batch_size=64,
                          validation_data=(x_test, y_test), verbose=1)

print("\nTraining Dropout Model...")
history_dropout = model_dropout.fit(x_train, y_train, epochs=5, batch_size=64,
                                    validation_data=(x_test, y_test), verbose=1)

# 5️⃣ Compare accuracy and loss
plt.figure(figsize=(12,5))

# ---- Loss comparison ----
plt.subplot(1,2,1)
plt.plot(history_base.history['val_loss'], label='Base Model')
plt.plot(history_l2.history['val_loss'], label='L2 Regularization')
plt.plot(history_dropout.history['val_loss'], label='Dropout')
plt.title('Validation Loss Comparison')
plt.xlabel('Epoch')
plt.ylabel('Loss')
plt.legend()

# ---- Accuracy comparison ----
plt.subplot(1,2,2)
plt.plot(history_base.history['val_accuracy'], label='Base Model')
plt.plot(history_l2.history['val_accuracy'], label='L2 Regularization')
plt.plot(history_dropout.history['val_accuracy'], label='Dropout')
plt.title('Validation Accuracy Comparison')
plt.xlabel('Epoch')
plt.ylabel('Accuracy')
plt.legend()

plt.show()

# 6️⃣ Print final test accuracies
print(f"\nBase Model Accuracy: {history_base.history['val_accuracy'][-1]*100:.2f}%")
print(f"L2 Regularized Model Accuracy: {history_l2.history['val_accuracy'][-1]*100:.2f}%")
print(f"Dropout Model Accuracy: {history_dropout.history['val_accuracy'][-1]*100:.2f}%")
