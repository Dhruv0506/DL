# 1Ô∏è‚É£ Import Libraries
import tensorflow as tf
from tensorflow.keras import layers, models, preprocessing
import matplotlib.pyplot as plt
import numpy as np

# 2Ô∏è‚É£ Load IMDB dataset
# Each review is already tokenized into integers representing words
vocab_size = 10000  # keep top 10,000 most common words
max_length = 200    # pad or truncate reviews to 200 words
(x_train, y_train), (x_test, y_test) = tf.keras.datasets.imdb.load_data(num_words=vocab_size)

# Pad sequences to make all reviews the same length
x_train = tf.keras.preprocessing.sequence.pad_sequences(x_train, maxlen=max_length)
x_test = tf.keras.preprocessing.sequence.pad_sequences(x_test, maxlen=max_length)

# 3Ô∏è‚É£ Build the LSTM Model
model = models.Sequential([
    layers.Embedding(input_dim=vocab_size, output_dim=64, input_length=max_length),
    layers.LSTM(64, return_sequences=False),
    layers.Dense(64, activation='relu'),
    layers.Dropout(0.5),
    layers.Dense(1, activation='sigmoid')  # output: probability of positive sentiment
])

model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])
model.summary()

# 4Ô∏è‚É£ Train the Model
history = model.fit(x_train, y_train,
                    epochs=5,
                    batch_size=64,
                    validation_data=(x_test, y_test))

# 5Ô∏è‚É£ Plot Training and Validation Curves
plt.figure(figsize=(12,5))

# Accuracy curve
plt.subplot(1,2,1)
plt.plot(history.history['accuracy'], label='Train Accuracy')
plt.plot(history.history['val_accuracy'], label='Validation Accuracy')
plt.title('Accuracy Curve')
plt.xlabel('Epoch')
plt.ylabel('Accuracy')
plt.legend()

# Loss curve
plt.subplot(1,2,2)
plt.plot(history.history['loss'], label='Train Loss')
plt.plot(history.history['val_loss'], label='Validation Loss')
plt.title('Loss Curve')
plt.xlabel('Epoch')
plt.ylabel('Loss')
plt.legend()

plt.show()

# 6Ô∏è‚É£ Evaluate the model
loss, acc = model.evaluate(x_test, y_test, verbose=0)
print(f"\n‚úÖ Test Accuracy: {acc*100:.2f}%")

# 7Ô∏è‚É£ Visualize Example Predictions
word_index = tf.keras.datasets.imdb.get_word_index()
reverse_word_index = {v+3: k for k, v in word_index.items()}
reverse_word_index[0], reverse_word_index[1], reverse_word_index[2] = '<PAD>', '<START>', '<UNK>'

def decode_review(encoded_review):
    return ' '.join([reverse_word_index.get(i, '?') for i in encoded_review])

# Pick a few random reviews to test
for i in range(3):
    idx = np.random.randint(0, len(x_test))
    review = decode_review(x_test[idx])
    pred = model.predict(np.array([x_test[idx]]))[0][0]
    sentiment = "üòä Positive" if pred > 0.5 else "üò° Negative"
    print(f"\nReview {i+1}: {review[:300]}...")
    print(f"Predicted Sentiment: {sentiment} (Confidence: {pred:.2f})")
