# Import libraries
import tensorflow as tf
from tensorflow.keras import layers, models
import matplotlib.pyplot as plt
import numpy as np
import time

# 1Ô∏è‚É£ Load and preprocess MNIST dataset
(x_train, y_train), (x_test, y_test) = tf.keras.datasets.mnist.load_data()
x_train, x_test = x_train / 255.0, x_test / 255.0

# 2Ô∏è‚É£ Function to build MLP with a given activation
def build_mlp(activation_fn):
    model = models.Sequential([
        layers.Flatten(input_shape=(28, 28)),
        layers.Dense(256, activation=activation_fn),
        layers.Dense(128, activation=activation_fn),
        layers.Dense(10, activation='softmax')
    ])
    model.compile(optimizer='adam',
                  loss='sparse_categorical_crossentropy',
                  metrics=['accuracy'])
    return model

# 3Ô∏è‚É£ Create models with different activations
activations = ['sigmoid', 'tanh', 'relu']
histories = {}
train_times = {}

# 4Ô∏è‚É£ Train and time each model
for act in activations:
    print(f"\nüîπ Training model with {act.upper()} activation...")
    model = build_mlp(act)
    start = time.time()
    history = model.fit(x_train, y_train,
                        epochs=5, batch_size=64,
                        validation_data=(x_test, y_test),
                        verbose=0)
    end = time.time()

    train_times[act] = end - start
    histories[act] = history
    print(f"{act.upper()} model trained in {train_times[act]:.2f} seconds")

# 5Ô∏è‚É£ Plot Training Loss and Accuracy
plt.figure(figsize=(12,5))

# Loss comparison
plt.subplot(1,2,1)
for act in activations:
    plt.plot(histories[act].history['val_loss'], label=f'{act.upper()}')
plt.title('Validation Loss Comparison')
plt.xlabel('Epoch'); plt.ylabel('Loss')
plt.legend()

# Accuracy comparison
plt.subplot(1,2,2)
for act in activations:
    plt.plot(histories[act].history['val_accuracy'], label=f'{act.upper()}')
plt.title('Validation Accuracy Comparison')
plt.xlabel('Epoch'); plt.ylabel('Accuracy')
plt.legend()

plt.show()

# 6Ô∏è‚É£ Compare training times
for act in activations:
    print(f"{act.upper()} training time: {train_times[act]:.2f} seconds")

# 7Ô∏è‚É£ Visualize gradient flow for one batch
# Take one batch of data
x_batch = x_train[:128]
y_batch = y_train[:128]

gradients = {}

for act in activations:
    model = build_mlp(act)
    with tf.GradientTape() as tape:
        preds = model(x_batch)
        loss = tf.keras.losses.sparse_categorical_crossentropy(y_batch, preds)
    grads = tape.gradient(loss, model.trainable_variables)
    grad_magnitudes = [tf.reduce_mean(tf.abs(g)).numpy() for g in grads if g is not None]
    gradients[act] = np.mean(grad_magnitudes)

# Plot gradient magnitudes
plt.figure(figsize=(6,4))
plt.bar(gradients.keys(), gradients.values(), color=['blue','orange','green'])
plt.title("Average Gradient Magnitude per Activation")
plt.ylabel("Mean |Gradient|")
plt.xlabel("Activation Function")
plt.show()
